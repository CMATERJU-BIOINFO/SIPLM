{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QD0_dMCr4aaP"
      },
      "outputs": [],
      "source": [
        "!pip install --use-deprecated legacy-resolver proteinbert-with-pretrain-model/Markdown-3.4.1-py3-none-any.whl\" 2>/dev/null\n",
        "!pip install --ignore-requires-python proteinbert-with-pretrain-model/google_auth-1.6.3-py2.py3-none-any.whl 2>/dev/null\n",
        "!pip install --ignore-requires-python proteinbert-with-pretrain-model/tensorboard-2.6.0-py3-none-any.whl 2>/dev/null\n",
        "!pip install --use-deprecated legacy-resolver proteinbert-with-pretrain-model/pyfaidx-0.7.1-py3-none-any.whl 2>/dev/null\n",
        "!pip install --use-deprecated legacy-resolver proteinbert-with-pretrain-model/typing_extensions-3.10.0.2-py3-none-any.whl 2>/dev/null\n",
        "!pip install --use-deprecated legacy-resolver proteinbert-with-pretrain-model/protein_bert-1.0.1-py3-none-any.whl 2>/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0qhI-L28CTO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from proteinbert import OutputType, OutputSpec, FinetuningModelGenerator, load_pretrained_model, finetune, evaluate_by_len, log, conv_and_global_attention_model\n",
        "from proteinbert.conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve,  confusion_matrix, f1_score, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from proteinbert.model_generation import load_pretrained_model_from_dump"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSg_G6A28Nxe"
      },
      "outputs": [],
      "source": [
        "BENCHMARKS_DIR = 'Dataset/biogrid_human/biogrid_human_dataset'\n",
        "BENCHMARKS = [\n",
        "    # name, output_type\n",
        "    ('Biogrid_human_1', OutputType(False, 'binary')),\n",
        "    ('Biogrid_human_2', OutputType(False, 'binary')),\n",
        "    ('Biogrid_human_3', OutputType(False, 'binary')),\n",
        "    ('Biogrid_human_4', OutputType(False, 'binary')),\n",
        "    ('Biogrid_human_5', OutputType(False, 'binary')),\n",
        "    ('Biogrid_human_6', OutputType(False, 'binary')),\n",
        "    ('Biogrid_human_7', OutputType(False, 'binary')),\n",
        "    ('Biogrid_human_8', OutputType(False, 'binary')),\n",
        "    ('Biogrid_human_9', OutputType(False, 'binary')),\n",
        "    ('Biogrid_human_10', OutputType(False, 'binary')),\n",
        "]\n",
        "\n",
        "settings = {\n",
        "    'max_dataset_size': None,\n",
        "    'max_epochs_per_stage': 40,\n",
        "    'seq_len': 256,\n",
        "    'batch_size': 32,\n",
        "    'final_epoch_seq_len': 1024,\n",
        "    'initial_lr_with_frozen_pretrained_layers': 1e-02,\n",
        "    'initial_lr_with_all_layers': 1e-04,\n",
        "    'final_epoch_lr': 1e-05,\n",
        "    'dropout_rate': 0.5,\n",
        "    'training_callbacks': [\n",
        "        keras.callbacks.ReduceLROnPlateau(patience = 1, factor = 0.25, min_lr = 1e-05, verbose = 1),\n",
        "        keras.callbacks.EarlyStopping(patience = 2, restore_best_weights = True),\n",
        "    ],\n",
        "}\n",
        "\n",
        "####### Uncomment for debug mode\n",
        "# settings['max_dataset_size'] = 500\n",
        "# settings['max_epochs_per_stage'] = 1\n",
        "\n",
        "def run_benchmark(benchmark_name, pretraining_model_generator, input_encoder, pretraining_model_manipulation_function = None):\n",
        "\n",
        "    log('========== %s ==========' % benchmark_name)\n",
        "\n",
        "    output_type = get_benchmark_output_type(benchmark_name)\n",
        "    log('Output type: %s' % output_type)\n",
        "\n",
        "    train_set, valid_set = load_benchmark_dataset(benchmark_name)\n",
        "    test_set = pd.read_csv(\"Dataset/biogrid_human/biogrid_human_dataset/Biogrid_human.test.csv\")\n",
        "    log(f'{len(train_set)} training set records, {len(valid_set)} validation set records, {len(test_set)} test set records.')\n",
        "\n",
        "    if settings['max_dataset_size'] is not None:\n",
        "        log('Limiting the training, validation and test sets to %d records each.' % settings['max_dataset_size'])\n",
        "        train_set = train_set.sample(min(settings['max_dataset_size'], len(train_set)), random_state = 0)\n",
        "        valid_set = valid_set.sample(min(settings['max_dataset_size'], len(valid_set)), random_state = 0)\n",
        "        test_set = test_set.sample(min(settings['max_dataset_size'], len(test_set)), random_state = 0)\n",
        "\n",
        "    if output_type.is_seq or output_type.is_categorical:\n",
        "        train_set['label'] = train_set['label'].astype(str)\n",
        "        valid_set['label'] = valid_set['label'].astype(str)\n",
        "        test_set['label'] = test_set['label'].astype(str)\n",
        "    else:\n",
        "        train_set['label'] = train_set['label'].astype(float)\n",
        "        valid_set['label'] = valid_set['label'].astype(float)\n",
        "        test_set['label'] = test_set['label'].astype(float)\n",
        "\n",
        "    if output_type.is_categorical:\n",
        "\n",
        "        if output_type.is_seq:\n",
        "            unique_labels = sorted(set.union(*train_set['label'].apply(set)) | set.union(*valid_set['label'].apply(set)) | \\\n",
        "                    set.union(*test_set['label'].apply(set)))\n",
        "        else:\n",
        "            unique_labels = sorted(set(train_set['label'].unique()) | set(valid_set['label'].unique()) | set(test_set['label'].unique()))\n",
        "\n",
        "        log('%d unique lebels.' % len(unique_labels))\n",
        "    elif output_type.is_binary:\n",
        "        unique_labels = [0, 1]\n",
        "    else:\n",
        "        unique_labels = None\n",
        "\n",
        "    output_spec = OutputSpec(output_type, unique_labels)\n",
        "\n",
        "\n",
        "    model_generator = FinetuningModelGenerator(pretraining_model_generator, output_spec, pretraining_model_manipulation_function = \\\n",
        "            pretraining_model_manipulation_function, dropout_rate = settings['dropout_rate'])\n",
        "    finetune(model_generator, input_encoder, output_spec, train_set['seq'], train_set['label'], valid_set['seq'], valid_set['label'], \\\n",
        "            seq_len = settings['seq_len'], batch_size = settings['batch_size'], max_epochs_per_stage = settings['max_epochs_per_stage'], \\\n",
        "            lr = settings['initial_lr_with_all_layers'], begin_with_frozen_pretrained_layers = True, lr_with_frozen_pretrained_layers = \\\n",
        "            settings['initial_lr_with_frozen_pretrained_layers'], n_final_epochs = 1, final_seq_len = settings['final_epoch_seq_len'], \\\n",
        "            final_lr = settings['final_epoch_lr'], callbacks = settings['training_callbacks'])\n",
        "\n",
        "    for dataset_name, dataset in [('Training-set', train_set), ('Validation-set', valid_set), ('Test-set', test_set)]:\n",
        "\n",
        "        log('*** %s performance: ***' % dataset_name)\n",
        "        results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, output_spec, dataset['seq'], dataset['label'], \\\n",
        "                start_seq_len = settings['seq_len'], start_batch_size = settings['batch_size'])\n",
        "\n",
        "        with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "            display(results)\n",
        "\n",
        "        if confusion_matrix is not None:\n",
        "            with pd.option_context('display.max_rows', 16, 'display.max_columns', 10):\n",
        "                log('Confusion matrix:')\n",
        "                display(confusion_matrix)\n",
        "        X = input_encoder.encode_X(test_set['seq'], 256)\n",
        "        model = model_generator.create_model(256)\n",
        "        y_pred = model.predict(X, batch_size = 32)\n",
        "\n",
        "\n",
        "        test_set['pred_label'] = y_pred.flatten()\n",
        "        test_set[['Protein','label', 'pred_label']].to_csv(f'Result/{benchmark_name}.csv', index=False)\n",
        "\n",
        "    return model_generator\n",
        "\n",
        "def load_benchmark_dataset(benchmark_name):\n",
        "\n",
        "    train_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.train.csv' % benchmark_name)\n",
        "    valid_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.valid.csv' % benchmark_name)\n",
        "    #test_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.test.csv' % benchmark_name)\n",
        "\n",
        "    train_set = pd.read_csv(train_set_file_path).dropna().drop_duplicates()\n",
        "    #test_set = pd.read_csv(test_set_file_path).dropna().drop_duplicates()\n",
        "\n",
        "    if os.path.exists(valid_set_file_path):\n",
        "        valid_set = pd.read_csv(valid_set_file_path).dropna().drop_duplicates()\n",
        "    else:\n",
        "        log(f'Validation set {valid_set_file_path} missing. Splitting training set instead.')\n",
        "        train_set, valid_set = train_test_split(train_set, stratify = train_set['label'], test_size = 0.1, random_state = 0)\n",
        "\n",
        "    return train_set, valid_set, #test_set\n",
        "\n",
        "def get_benchmark_output_type(benchmark_name):\n",
        "    for name, output_type in BENCHMARKS:\n",
        "        if name == benchmark_name:\n",
        "            return output_type\n",
        "\n",
        "pretrained_model_generator, input_encoder = load_pretrained_model_from_dump(\n",
        "    \"proteinbert-with-pretrain-model/epoch_92400_sample_23500000.pkl\",\n",
        "    conv_and_global_attention_model.create_model)\n",
        "\n",
        "for benchmark_name, _ in BENCHMARKS:\n",
        "    run_benchmark(benchmark_name, pretrained_model_generator, input_encoder, pretraining_model_manipulation_function = \\\n",
        "            get_model_with_hidden_layers_as_outputs)\n",
        "\n",
        "log('Done.')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
