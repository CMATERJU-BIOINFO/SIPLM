{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0tZaglXHqDU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve,  confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Read the CSV file\n",
        "df = pd.read_csv(\"Result/Biogrid_human_1.csv\")\n",
        "\n",
        "# Step 2: Aggregate the predictions\n",
        "# Group by 'Protein' and aggregate 'Pred_label' using majority voting\n",
        "# and 'Y_Prob' using mean value\n",
        "agg_df = df.groupby('Protein').agg(\n",
        "    label=('label', 'first'),  # Assuming 'label' is the same for all sub-sequences of a protein\n",
        "    Pred_label=('Pred_label', lambda x: x.mode().iloc[0]),  # Majority voting\n",
        "    Y_Prob=('Y_Prob', 'mean')  # Mean value\n",
        ").reset_index()\n",
        "agg_df['Y_Prob_thres'] = (agg_df['Y_Prob'] >= 0.5).astype(int)\n",
        "# Step 3: Calculate performance metrics\n",
        "y_true = agg_df['label']\n",
        "y_pred = agg_df['Pred_label']\n",
        "y_prob = agg_df['Y_Prob']\n",
        "y_prob_thres = agg_df['Y_Prob_thres']\n",
        "# Calculate metrics\n",
        "\n",
        "print(\"Majority voting\")\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_true, y_prob_thres)\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(y_true, y_prob_thres)\n",
        "\n",
        "# Recall (Sensitivity)\n",
        "recall = recall_score(y_true, y_prob_thres)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_true, y_prob_thres).ravel()\n",
        "spec = tn / (tn + fp)\n",
        "print(f\"Specificity: {spec}\")\n",
        "#confusion matrix\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "# AUC-ROC curve\n",
        "auc_roc = roc_auc_score(y_true, y_prob)\n",
        "\n",
        "# Plot AUC-ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc_roc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "f1 = f1_score(y_true, y_prob_thres)\n",
        "print(f\"F1-score: {f1}\")\n",
        "# Print the metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall (Sensitivity): {recall}\")\n",
        "print(f\"AUC-ROC: {auc_roc}\")\n",
        "\n",
        "# Save the aggregated results to a new CSV file\n",
        "agg_df.to_csv('Result/seq_level_predictions.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ni9ZpnZH1Uu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix, roc_curve, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the subsequence-level predictions\n",
        "test_df = pd.read_csv('Result/Biogrid_human_1.csv')\n",
        "\n",
        "# Sequence-level aggregation\n",
        "seq_level_df = test_df.groupby('Protein').agg({\n",
        "    'label': 'first',  # Take the first label as they should be the same across subsequences\n",
        "    'Pred_label': lambda x: 1 if 1 in x.values else 0,  # One-hit rule\n",
        "    'Y_Prob': 'mean'\n",
        "      # Conditional adjustment based on the mean\n",
        "}).reset_index()\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "y_true = seq_level_df['label']\n",
        "y_pred = seq_level_df['Pred_label']\n",
        "y_prob = seq_level_df['Y_Prob']\n",
        "\n",
        "# Metrics\n",
        "print(\"One hit\")\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "auc_roc = roc_auc_score(y_true, y_prob)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "spec = tn / (tn + fp)\n",
        "print(f\"Specificity: {spec}\")\n",
        "print(f\"F1-score: {f1}\")\n",
        "\n",
        "# Confusion matrix for sensitivity (Recall is equivalent to sensitivity for positive class)\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall (Sensitivity): {recall}\")\n",
        "print(f\"AUC-ROC: {auc_roc}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "\n",
        "seq_level_df.to_csv('seq_level_predictions_OH.csv', index=False)\n",
        "\n",
        "\n",
        "# Plot AUC-ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='blue', label=f'AUC-ROC = {auc_roc}')\n",
        "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('AUC-ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFcAxHfoH4Sc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Calculate J = TPR - FPR and find the optimal threshold\n",
        "J = tpr - fpr\n",
        "optimal_idx = np.argmax(J)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "\n",
        "print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
        "\n",
        "# Thresholding using the optimal threshold\n",
        "y_pred_thresholded = (y_prob >= optimal_threshold).astype(int)\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred_thresholded)\n",
        "\n",
        "print(\"\\nConfusion Matrix after Thresholding:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_true, y_pred_thresholded).ravel()\n",
        "spec = tn / (tn + fp)\n",
        "print(f\"Specificity: {spec}\")\n",
        "accuracy = accuracy_score(y_true, y_pred_thresholded)\n",
        "precision = precision_score(y_true, y_pred_thresholded)\n",
        "recall = recall_score(y_true, y_pred_thresholded)\n",
        "f1 = f1_score(y_true, y_pred_thresholded)\n",
        "print(f\"F1-score: {f1}\")\n",
        "# Print results\n",
        "print(\"Accuracy from Y_pred\")\n",
        "print(f\"Accuracy: {accuracy:.8f}\")\n",
        "print(f\"Precision: {precision:.8f}\")\n",
        "print(f\"Recall: {recall:.8f}\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='blue', label=f'AUC-ROC = {auc_roc}')\n",
        "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('AUC-ROC Curve with Optimal Threshold')\n",
        "\n",
        "# Plot the optimal threshold point\n",
        "plt.plot(fpr[optimal_idx], tpr[optimal_idx], marker='o', color='black', label=f'Optimal Threshold: {optimal_threshold:.4f}')\n",
        "\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcyDXQmtH8kA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
